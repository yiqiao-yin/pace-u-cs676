{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo_7CvY--Egh"
      },
      "source": [
        "## tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-IKZY329488"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + tf.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URQ2ru8w9zoB"
      },
      "outputs": [],
      "source": [
        "def linear_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Computes the linear combination of inputs and weights.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    W: tf.Tensor - Weights\n",
        "    b: tf.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Linear combination\n",
        "    \"\"\"\n",
        "    return tf.linalg.matvec(X, W) + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4ILN7M492zO"
      },
      "outputs": [],
      "source": [
        "def logistic_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Logistic regression model: Applies sigmoid activation on linear regression output.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    W: tf.Tensor - Weights\n",
        "    b: tf.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Predicted probabilities\n",
        "    \"\"\"\n",
        "    z = linear_regression(X, W, b)\n",
        "    return sigmoid(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_fA36n097Lp"
      },
      "outputs": [],
      "source": [
        "def calc_bce_loss(y, y_hat):\n",
        "    \"\"\"\n",
        "    Computes binary cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    y: tf.Tensor - True labels\n",
        "    y_hat: tf.Tensor - Predicted probabilities\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Loss value\n",
        "    \"\"\"\n",
        "    epsilon = 1e-7  # Prevent log(0)\n",
        "    y_hat = tf.clip_by_value(y_hat, epsilon, 1 - epsilon)\n",
        "    return -tf.reduce_mean(y * tf.math.log(y_hat) + (1 - y) * tf.math.log(1 - y_hat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_x8Q_MU9_gd"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, W, b, lr):\n",
        "    \"\"\"\n",
        "    Updates model parameters using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    y: tf.Tensor - True labels\n",
        "    W: tf.Variable - Weights\n",
        "    b: tf.Variable - Bias\n",
        "    lr: float - Learning rate\n",
        "\n",
        "    Returns:\n",
        "    tf.Variable, tf.Variable - Updated weights and bias\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_hat = logistic_regression(X, W, b)\n",
        "        loss = calc_bce_loss(y, y_hat)\n",
        "\n",
        "    gradients = tape.gradient(loss, [W, b])\n",
        "    W.assign_sub(lr * gradients[0])\n",
        "    b.assign_sub(lr * gradients[1])\n",
        "    return W, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpzx-yOr-A0o"
      },
      "outputs": [],
      "source": [
        "def predict(X, W, b, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Makes predictions based on logistic regression.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    W: tf.Tensor - Weights\n",
        "    b: tf.Tensor - Bias\n",
        "    threshold: float - Threshold for classification\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Predicted classes (0 or 1)\n",
        "    \"\"\"\n",
        "    y_hat = logistic_regression(X, W, b)\n",
        "    return tf.cast(y_hat >= threshold, dtype=tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk7WJ1-a-Ck3",
        "outputId": "d9f38036-a17f-4042-ef78-ef306d9f51dd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Toy dataset\n",
        "X = tf.constant([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]], dtype=tf.float32)  # Input features\n",
        "y = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32)  # True labels\n",
        "\n",
        "# Initialize parameters\n",
        "W = tf.Variable([0.1, -0.1], dtype=tf.float32)  # Weights\n",
        "b = tf.Variable(0.0, dtype=tf.float32)  # Bias\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Number of iterations\n",
        "num_iterations = 1000\n",
        "\n",
        "# Step-by-step test\n",
        "print(\"Initial Weights:\", W.numpy())\n",
        "print(\"Initial Bias:\", b.numpy())\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    W, b = gradient_descent(X, y, W, b, lr)\n",
        "\n",
        "    if i % 100 == 0:  # Print progress every 100 iterations\n",
        "        y_hat = logistic_regression(X, W, b)\n",
        "        loss = calc_bce_loss(y, y_hat)\n",
        "        print(f\"Iteration {i}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Final parameters\n",
        "print(\"Final Weights:\", W.numpy())\n",
        "print(\"Final Bias:\", b.numpy())\n",
        "\n",
        "# Predictions\n",
        "predictions = predict(X, W, b, threshold=0.5)\n",
        "print(\"Predictions:\", predictions.numpy())\n",
        "\n",
        "# Check final probabilities\n",
        "probabilities = logistic_regression(X, W, b)\n",
        "print(\"Predicted Probabilities:\", probabilities.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyRaZXYP-GEI"
      },
      "source": [
        "## torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtZ3ur9E-Gnm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + torch.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApszeSx5-HDP"
      },
      "outputs": [],
      "source": [
        "def linear_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Computes the linear combination of inputs and weights.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Linear combination\n",
        "    \"\"\"\n",
        "    return X @ W + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F39xmmAH-Ik3"
      },
      "outputs": [],
      "source": [
        "def logistic_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Logistic regression model: Applies sigmoid activation on linear regression output.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Predicted probabilities\n",
        "    \"\"\"\n",
        "    z = linear_regression(X, W, b)\n",
        "    return sigmoid(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jnxvV-g-J0L"
      },
      "outputs": [],
      "source": [
        "def calc_bce_loss(y, y_hat):\n",
        "    \"\"\"\n",
        "    Computes binary cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    y: torch.Tensor - True labels\n",
        "    y_hat: torch.Tensor - Predicted probabilities\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Loss value\n",
        "    \"\"\"\n",
        "    epsilon = 1e-7  # Prevent log(0)\n",
        "    y_hat = torch.clamp(y_hat, epsilon, 1 - epsilon)\n",
        "    return -torch.mean(y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK4yIrJh-LDI"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, W, b, lr):\n",
        "    \"\"\"\n",
        "    Updates model parameters using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    y: torch.Tensor - True labels\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "    lr: float - Learning rate\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor, torch.Tensor - Updated weights and bias\n",
        "    \"\"\"\n",
        "    y_hat = logistic_regression(X, W, b)\n",
        "    loss = calc_bce_loss(y, y_hat)\n",
        "\n",
        "    loss.backward()  # Compute gradients\n",
        "\n",
        "    with torch.no_grad():\n",
        "        W -= lr * W.grad\n",
        "        b -= lr * b.grad\n",
        "\n",
        "        # Zero gradients for the next step\n",
        "        W.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "    return W, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN2t7tfW-NvT"
      },
      "outputs": [],
      "source": [
        "def predict(X, W, b, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Makes predictions based on logistic regression.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "    threshold: float - Threshold for classification\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Predicted classes (0 or 1)\n",
        "    \"\"\"\n",
        "    y_hat = logistic_regression(X, W, b)\n",
        "    return (y_hat >= threshold).int()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk_6VLSd-PBf",
        "outputId": "ba33bc89-b36a-4bab-e13f-746ec47ce14c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Toy dataset\n",
        "X = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]], dtype=torch.float32)  # Input features\n",
        "y = torch.tensor([0.0, 0.0, 1.0, 1.0], dtype=torch.float32)  # True labels\n",
        "\n",
        "# Initialize parameters\n",
        "W = torch.tensor([0.1, -0.1], dtype=torch.float32, requires_grad=True)  # Weights\n",
        "b = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)  # Bias\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Number of iterations\n",
        "num_iterations = 1000\n",
        "\n",
        "# Test training process\n",
        "print(\"Initial Weights:\", W.detach().numpy())\n",
        "print(\"Initial Bias:\", b.detach().numpy())\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # Perform gradient descent\n",
        "    W, b = gradient_descent(X, y, W, b, lr)\n",
        "\n",
        "    if i % 100 == 0:  # Print progress every 100 iterations\n",
        "        y_hat = logistic_regression(X, W, b)\n",
        "        loss = calc_bce_loss(y, y_hat)\n",
        "        print(f\"Iteration {i}, Loss: {loss.item()}\")\n",
        "\n",
        "# Final parameters\n",
        "print(\"Final Weights:\", W.detach().numpy())\n",
        "print(\"Final Bias:\", b.detach().numpy())\n",
        "\n",
        "# Predictions\n",
        "predictions = predict(X, W, b, threshold=0.5)\n",
        "print(\"Predictions:\", predictions.numpy())\n",
        "\n",
        "# Check final probabilities\n",
        "probabilities = logistic_regression(X, W, b)\n",
        "print(\"Predicted Probabilities:\", probabilities.detach().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKQaYMM-_Upg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
